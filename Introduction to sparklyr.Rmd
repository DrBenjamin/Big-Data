---
title: "Introduction to sparklyr"
output:
  pdf_document:
    includes:
      in_header: header.tex
---

We will largely follow chapters **2** and **3** of **Mastering Spark with R**, [https://therinspark.com](https://therinspark.com/index.html).

First install the following packages if you do not already have them already, and load them with the `library()` function: 

```{r warning=FALSE, message=FALSE}
library(sparklyr)
library(dplyr)
library(ggplot2)
library(knitr)
```


## Preliminiaries

If you are working on EIDF, first make sure that the default working directory in RStudio is your folder. In RStudio, select Tools -> Global Options. Change the default working directory to be /work/eidf071/eidf071/<your username>.

To confirm the change has taken effect, close and then reopen RStudio, and type getpw() into the console. It should show your working directory correctly as above.

## Connecting

```{r}
sc = spark_connect(master = 'local')
```

The following code will take the built-in mtcars dataset, stored in an R dataframe, and put it into a spark dataframe. We will use this dataset in many of our examples.


```{r}
cars = copy_to(sc, mtcars, overwrite = TRUE)
```

## Data input/output

### Write to a csv file

This will create a folder in your working directory called `cars.csv`. It 
contains a csv with the cars data in it.

```{r}
spark_write_csv(cars, "cars.csv")
```

Note that running this more than once will result in an error because 
spark_write_csv will not overwrite a folder which is already created. You
may need to delete the folder before running the code again.

### Read from a csv file

```{r}
spark_read_csv(sc, 'cars.csv') %>%
  head() %>%
  kable()
```

## Data wrangling

Familiar commands from dplyr work as you would expect, but now they
instead connect to Spark and would be run in parallel across the cluster.

### Create a new column

```{r warning = FALSE}
cars = mutate(cars, transmission = ifelse(am == 0, 'automatic', 'manual'))
```

### Select columns

```{r}
select(cars, am, transmission) %>%
  head() %>%
  kable()
```

# Calculate the mean of each column

```{r}
summarise_all(cars, mean, na.rm = TRUE) %>%
  kable()
```

## Plots

Creating a plot isn't usually highly computationally demanding. Therefore, sparklyr
does not have a full-fledged equivalent of ggplot. It is typically best to perform all 
data manipulations in Spark, then bring the result back to R using the collect() 
command. Finally, we use the regular ggplot package to make the graph.

```{r}
# Data manipulations are done first using spark
car_group = cars %>%
  group_by(cyl) %>%
  summarise(mpg = sum(mpg, na.rm = TRUE)) %>%
  # `collect` brings the spark dataframe back to a regular R dataframe
  collect()

# Now use ggplot on the R dataframe car_group
ggplot(aes(as.factor(cyl), mpg), data = car_group) +
  geom_col(fill = 'SteelBlue') +
  xlab('Cylinders') +
  coord_flip()
```

```{r}
install.packages("nycflights13", "Lahman")
library(dplyr)
library(ggplot2)
library(jsonlite)

iris_tbl <- copy_to(sc, iris, overwrite = TRUE)
flights_tbl <- copy_to(sc, nycflights13::flights, "flights", overwrite = TRUE)
batting_tbl <- copy_to(sc, Lahman::Batting, "batting", overwrite = TRUE)
src_tbls(sc)

flights_tbl %>% filter(dep_delay == 2)

delay <- flights_tbl %>% 
  group_by(tailnum) %>%
  summarise(count = n(), dist = mean(distance, na.rm = TRUE), delay = mean(arr_delay, na.rm = TRUE)) %>%
  filter(count > 20, dist < 2000, !is.na(delay)) %>%
  collect()

# Saving as json
#write_json(delay, "file.json")

ggplot(delay, aes(dist, delay)) +
  geom_point(aes(size = count), alpha = 1/2) +
  geom_smooth() +
  scale_size_area(max_size = 2)
```

## Models, in brief

We will go into more details about these models in coming lectures.

### OLS

```{r}
ols_model = ml_linear_regression(cars, mpg ~ hp + disp)
summary(ols_model)
```

## Logistic regression

The command `ml_logistic_regression` can be used to train a multinomial model, where
the dependent variable has more than two categories. However, it does not report
standard deviations of parameter estimates.

```{r}
lr_model = ml_logistic_regression(cars, am ~ hp + disp)
summary(lr_model)
```

The command `ml_generalized_linear_regression` can also be used to train a 
logistic model with binary variable, but **dependent variables with more than
two categories are not supported!** However, it does report standard deviations
of parameter estimates.

```{r}
lr_model = ml_generalized_linear_regression(cars, am ~ hp + disp, family = 'binomial')
summary(lr_model)
```

### Multilayer perceptron

```{r}
mlp_model = ml_multilayer_perceptron_classifier(
  cars,
  am ~ hp + disp,
  layers = c(2, 8, 8, 2)
)
predictions = ml_predict(mlp_model, cars)

select(predictions, prediction, probability_0, probability_1) %>%
  head() %>%
  kable()
```

 ### Gradient boosted tress
 
 Classification trees:
 
```{r}
 gbt_model = ml_gradient_boosted_trees(cars, am ~ hp + disp, type = 'classification')
 predictions = ml_predict(gbt_model, cars)
 
 select(predictions, prediction, probability_0, probability_1) %>%
   head() %>%
   kable()
```

Regression trees:

```{r}
gbt_model = ml_gradient_boosted_trees(cars, mpg ~ hp + disp, type = 'regression')
predictions = ml_predict(gbt_model, cars)

select(predictions, mpg, prediction) %>%
  head() %>%
  kable()
```

### Other models

Apache Spark supports many other models - I have just chosen a few to look at
more closely. I encourage you to explorer others! See documentation here:
[https://spark.apache.org/docs/latest/ml-classification-regression.html](https://spark.apache.org/docs/latest/ml-classification-regression.html)

## Disconnecting

The following code chukn disconnects from the cluster. You should always do this
after your job has been run.

```{r}
spark_disconnect(sc)
```
