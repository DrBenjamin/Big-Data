---
title: "Machine learning modelling with sparklyr"
output: pdf_document
---

**Week 5 tutorial task**

In this exercise, we will be exploring some machine learning models with sparklyr.

Load up the required packages.

```{r warning=FALSE, message=FALSE}
# Load required libraries
library(sparklyr)
library(dplyr)
library(ggplot2)
library(knitr)
library(broom)
library(httr)
library(aws.s3)
```

Create a spark connection that treats your local machine as a cluster. Load up the built-in R dataset mtcars into a Spark dataframe.

```{r warning = FALSE}
sc <- spark_connect(master = "local")
cars <- copy_to(sc, mtcars, overwrite = TRUE)
```


# Task 1

One-hot encode the variable cyl, and create a new column called gear_relabelled,
that relabels the gear variable so that it consists of numbers starting at 0. You
may find the case_when function useful (Google it).

```{r}
## My solution
cars_cyl <- cars %>%
  mutate(
    cyl_4 = ifelse(cyl == 4, 1, 0),
    cyl_6 = ifelse(cyl == 6, 1, 0),
    cyl_8 = ifelse(cyl == 8, 1, 0)
  ) %>%
  mutate(gear_relabelled = gear - min(gear, na.rm = TRUE))

cars_cyl %>%
  select(gear_relabelled) %>%
  pull() %>%
  max()

cars_cyl %>%
  collect() %>%
  kable()
```


# Task 2

Create a multinomial logistic regression model that predicts the number of gears,
using hp and number of cylinders as predictor variables. **Remember to not fall
into the dummy variable trap!** 
Apply the tidy function to the model, and display the result of doing this 
in a table. What do the entries in the table mean? What is their interpretation?
Is there anything that was in the results table of the OLS model that is not
present in the results table for this model?

```{r  warning = FALSE}
## My solution
mlr_model <- cars_cyl %>%
  ml_logistic_regression(gear_relabelled ~ hp + cyl_6 + cyl_8)

mlr_model_tidy <- tidy(mlr_model)

mlr_model_tidy %>%
  collect() %>%
  kable()

ols_model <- cars_cyl %>%
  ml_linear_regression(gear_relabelled ~ hp + cyl_4 + cyl_6 + cyl_8,
                       fit_intercept = FALSE)

# Apply the tidy function to the fitted model
ols_model_tidy <- tidy(ols_model)

# Display the results in a table
ols_model_tidy %>%
  collect() %>%
  kable()
```
**Results:** No standard deviations and p-values reported as seen in OLS (Ordinary least square).

# Task 3

Evaluate the multinomial logistic regression model. The main metrics that are
of interest are true positive rate, false positive rate, accuracy, precision and
recall. Print these metrics out. Do you notice anything about the true positive 
rate and the recall?

```{r warning = FALSE}
## Live session solution
evaluation_result <- ml_evaluate(lr_model, cars_df)

# Positive rates
tpr <- evaluation_result$true_positive_rate_by_label()
print(paste0("True positive rate: ", round(tpr * 100, 2), "%"))
fpr <- evaluation_result$false_positive_rate_by_label()
print(paste0("False positive rate: ", round(tpr * 100, 2), "%"))

# Other metrics
accuracy <- evaluation_result$accuracy()
print(paste0("Accuracy: ", round(accuracy * 100, 2), "%"))

precision <- evaluation_result$precision_by_label()
print(paste0("Precision: ", round(precision * 100, 2), "%"))

recall <- evaluation_result$recall_by_label()
print(paste0("Recall: ", round(recall * 100, 2), "%"))
```

```{r  warning = FALSE}
## My solution, gets some different results, but mostly nearby
# Get predictions from the model
predictions <- ml_predict(mlr_model, cars_cyl)
print(predictions)

# Generate confusion matrix
confusion_matrix <- predictions %>%
  group_by(gear_relabelled, prediction) %>%
  summarise(counts = n()) %>%
  collect()
print(confusion_matrix)

# Calculate true positive rate
rates <- confusion_matrix %>%
  mutate(gear = prediction + 3) %>%
  mutate(true = ifelse(gear_relabelled == prediction, counts, 0),
         false = ifelse(gear_relabelled != prediction, counts, 0)
  ) %>%
  select(-prediction) %>%
  group_by(gear) %>%
  summarise(true_positiv_rate = sum(true) / sum(counts) * 100,
            false_positiv_rate = sum(false) / sum(counts) * 100)
rates %>%
  collect() %>%
  kable()

# Calculate accuracy
accuracy <- ml_multiclass_classification_evaluator(
  predictions,
  label_col = "gear_relabelled",
  prediction_col = "prediction",
  metric_name = "accuracy"
)
print(paste0("Accuracy: ", round(accuracy * 100, 2), "%"))

precision <- ml_multiclass_classification_evaluator(
  predictions,
  label_col = "gear_relabelled",
  prediction_col = "prediction",
  metric_name = "weightedPrecision"
)
print(paste0("Precision: ", round(precision * 100, 2), "%"))

recall <- ml_multiclass_classification_evaluator(
  predictions,
  label_col = "gear_relabelled",
  prediction_col = "prediction",
  metric_name = "weightedRecall"
)
print(paste0("Recall: ", round(recall * 100, 2), "%"))
```

**Question:** Why doesn't it evaluate the tnr/specificty

# Task 4

Create a binary logistic regression model that predicts the variable am with hp and
vs as predictor variables, using ml_generalized_linear_regression. 
Apply the tidy function to the model, and display the result of doing this in a table.

```{r}
## My solution
# Binary dependents
blr_model <-
  ml_generalized_linear_regression(cars_cyl, family = "binomial", am ~ hp + vs)

blr_model_tidy <- blr_model %>%
  tidy()

blr_model_tidy %>%
  collect() %>%
  kable()
```


# Task 5

Recall that the odds ratio from a logistic regression model is given by the
exponential of a fitted coefficient. The 95% confidence interval for the odds ratio
is given by the exponential of the upper and lower limits of the 95% confidence
interval for the fitted coefficient.

In the results table, create three new columns for the odds ratio, and the upper
and lower limits of its 95% confidence interval. Then create a column
that has the odds ratio, followed by a bracket with the 95% confidence interval, 
with all numbers rounded to 2 decimal places. Display the results in a table.

```{r}
## Live session solution
blr_model_tidy %>%
  mutate(odds_ratio = exp(estimate),
         lower_CI = exp(estimate - 1.96 * std.error),
         upper_CI = exp(estimate + 1.96 * std.error),
         odds_ratio_CI = sprintf("%.2f (%.2f, %.2f)", odds_ratio, lower_CI, upper_CI)) %>% 
  print()
```


# Task 6

Access and print out the AIC value for the model.

```{r  warning = FALSE}
## Live session solution
blr_evaluate <- ml_evaluate(blr_model, cars_cyl)
blr_evaluate$aic()
```

Akaike Information Criterion measures is used in statistical modeling to compare different models and select the best one. The AIC provides a means for model selection by balancing the goodness of fit of the model and the complexity of the model.

# Task 7

Create a new column that is the z-score of hp.

```{r}
## Live session solution
cars_cyl <- cars_cyl %>% 
  mutate(z_hp = (hp - mean(hp, na.rm = TRUE)) / sd(hp, na.rm = TRUE))
  head(cars_df, 3) %>% 
    print()
```


# Task 8

Create a multilayer perceptron classifier that predicts the variable gear, 
using hp and number of cylinders as predictor variables. Extract predictions
from the model, and show the first few rows of predictions in a table. 
**Make sure that the number of neurons in the input and output layers are correct.
Make sure that continuous variables are normalised, and categorical variables
are one-hot encoded.**

```{r}
## Live session solution
mlp_model <- ml_multilayer_perceptron_classifier(cars_cyl,
                                                 gear_relabelled ~ z_hp + cyl_6 + cyl_8,
                                                 layers = c(3, 7, 7, 3)
                                                )
mlp_predict <- ml_predict(mlp_model, cars_df)
head(mlp_predict) %>% 
  print()
```

# Task 9

Use ml_evaluate to print out model metrics. Which metrics are available?

```{r}
## Live session solution
ml_evaluate(mlp_model, cars_cyl) %>%
  print()
```


# Task 10

Create a gradient boosted tree classifier that predicts the variable am, 
using hp and number of cylinders as predictor variables. Calculate the C
statistic using ml_binary_classification_evaluator.

```{r}
## Live session solution
gbt_model <- ml_gradient_boosted_trees(cars_cyl, am ~ hp + cyl, 
                                       type = 'classification')

predict_gbt <- ml_predict(gbt_model, cars_cyl)
ml_binary_classification_evaluator(predict_gbt, metric_name = "areaUnderROC")
```
